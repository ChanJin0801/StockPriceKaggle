{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1: import requests\n",
    "2: from bs4 import BeautifulSoup\n",
    "\n",
    "3: \n",
    "\n",
    "4:  # 기사의 링크들이 담기는 리스트입니다.\n",
    "5: rsss = []\n",
    "\n",
    "6: # 파일은 아래 폴더에 저장됩니다.\n",
    "\n",
    "7: fileOut = open('RssfileOut.txt','w', encoding='utf-8')\n",
    "\n",
    "8: # rss와 기사에서 특정 부분을 크롤링하는 함수입니다.\n",
    "\n",
    "9: def crawler(url, parser, css_selector):\n",
    "10:    r = requests.get(url)\n",
    "11:    soup = BeautifulSoup(r.content, parser)\n",
    "12:    datas = soup.select(css_selector)\n",
    "\n",
    "13:    if parser == 'lxml':\n",
    "14:        print(datas[0].text, file=fileOut)\n",
    "15:    else:\n",
    "16:        for data in datas:\n",
    "17:            rsss.append(data.text)\n",
    "\n",
    "18: # 실행코드\n",
    "19: print(\"크롤링을 시작합니다.\")\n",
    "\n",
    "20: crawler('http://file.mk.co.kr/news/rss/rss_50300009.xml','xml','item link')\n",
    "    https://www.mk.co.kr/news/it/\n",
    "\n",
    "21: print(\"rss 추출이 완료되었습니다.\")\n",
    "\n",
    "22: for link in rsss:\n",
    "23:    try:\n",
    "24:        crawler(link, 'lxml', '#article_body')\n",
    "25:        print(\"=\"*20)\n",
    "26:    except Exception as e:\n",
    "27:        print(e)\n",
    "28:        print('진행중이에요...')\n",
    "29:        continue\n",
    "\n",
    "30: print(\"크롤링을 종료합니다.\")\n",
    "31: fileOut.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
